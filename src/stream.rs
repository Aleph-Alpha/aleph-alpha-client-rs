use crate::{
    chat::ChatBody, completion::BodyCompletion, http::Task, Error, TaskChat, TaskCompletion,
};
use serde::Deserialize;

/// Describes a chunk of a completion stream
#[derive(Deserialize, Debug)]
pub struct StreamChunk {
    /// The index of the stream that this chunk belongs to.
    /// This is relevant if multiple completion streams are requested (see parameter n).
    pub index: u32,
    /// The completion of the stream.
    pub completion: String,
}

/// Denotes the end of a completion stream.
///
/// The index of the stream that is being terminated is not deserialized.
/// It is only relevant if multiple completion streams are requested, (see parameter n),
/// which is not supported by this crate yet.
#[derive(Deserialize, Debug)]
pub struct StreamSummary {
    /// Model name and version (if any) of the used model for inference.
    pub model_version: String,
    /// The reason why the model stopped generating new tokens.
    pub finish_reason: String,
}

/// Denotes the end of all completion streams.
#[derive(Deserialize, Debug)]
pub struct CompletionSummary {
    /// Number of tokens combined across all completion tasks.
    /// In particular, if you set best_of or n to a number larger than 1 then we report the
    /// combined prompt token count for all best_of or n tasks.
    pub num_tokens_prompt_total: u32,
    /// Number of tokens combined across all completion tasks.
    /// If multiple completions are returned or best_of is set to a value greater than 1 then
    /// this value contains the combined generated token count.
    pub num_tokens_generated: u32,
}

#[derive(Deserialize, Debug)]
#[serde(tag = "type")]
#[serde(rename_all = "snake_case")]
pub enum CompletionEvent {
    StreamChunk(StreamChunk),
    StreamSummary(StreamSummary),
    CompletionSummary(CompletionSummary),
}

/// Wrap a completion task and support streaming.
pub struct TaskStreamCompletion<'a> {
    pub task: TaskCompletion<'a>,
}

impl Task for TaskStreamCompletion<'_> {
    type Output = CompletionEvent;

    type ResponseBody = CompletionEvent;

    fn build_request(
        &self,
        client: &reqwest::Client,
        base: &str,
        model: &str,
    ) -> reqwest::RequestBuilder {
        let body = BodyCompletion::new(model, &self.task).with_streaming();
        client.post(format!("{base}/complete")).json(&body)
    }

    fn body_to_output(response: Self::ResponseBody) -> Self::Output {
        response
    }
}

#[derive(Deserialize, Debug)]
pub struct Message {
    /// The role of the current chat completion. Will be assistant for the first chunk of every
    /// completion stream and missing for the remaining chunks.
    pub role: Option<String>,
    /// The content of the current chat completion. Will be empty for the first chunk of every
    /// completion stream and non-empty for the remaining chunks.
    pub content: String,
}

/// One chunk of a chat completion stream.
#[derive(Deserialize, Debug)]
pub struct ChatStreamChunk {
    /// The reason the model stopped generating tokens.
    /// The value is only set in the last chunk of a completion and null otherwise.
    pub finish_reason: Option<String>,
    /// Chat completion chunk generated by the model when streaming is enabled.
    pub delta: Message,
}

/// Event received from a chat completion stream. As the crate does not support multiple
/// chat completions, there will always exactly one choice item.
#[derive(Deserialize, Debug)]
pub struct ChatEvent {
    pub choices: Vec<ChatStreamChunk>,
}

/// Wrap a chat task and support streaming.
pub struct TaskStreamChat<'a> {
    pub task: TaskChat<'a>,
}

impl<'a> Task for TaskStreamChat<'a> {
    type Output = ChatStreamChunk;

    type ResponseBody = ChatEvent;

    fn build_request(
        &self,
        client: &reqwest::Client,
        base: &str,
        model: &str,
    ) -> reqwest::RequestBuilder {
        let body = ChatBody::new(model, &self.task).with_streaming();
        client.post(format!("{base}/chat/completions")).json(&body)
    }

    fn body_to_output(mut response: Self::ResponseBody) -> Self::Output {
        // We always expect there to be exactly one choice, as the `n` parameter is not
        // supported by this crate.
        response
            .choices
            .pop()
            .expect("There must always be at least one choice")
    }
}

/// Take a byte slice (of a SSE) and parse it into a provided response body.
/// Each SSE event is expected to contain one or multiple JSON bodies prefixed by `data: `.
pub fn parse_stream_event<ResponseBody>(bytes: &[u8]) -> Vec<Result<ResponseBody, Error>>
where
    ResponseBody: for<'de> Deserialize<'de>,
{
    String::from_utf8_lossy(bytes)
        .split("data: ")
        .skip(1)
        .map(|s| {
            serde_json::from_str(s).map_err(|e| Error::StreamDeserializationError {
                cause: e.to_string(),
                event: s.to_string(),
            })
        })
        .collect()
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn stream_chunk_event_is_parsed() {
        // Given some bytes
        let bytes = b"data: {\"type\":\"stream_chunk\",\"index\":0,\"completion\":\" The New York Times, May 15\"}\n\n";

        // When they are parsed
        let events = parse_stream_event::<CompletionEvent>(bytes);
        let event = events.first().unwrap().as_ref().unwrap();

        // Then the event is a stream chunk
        match event {
            CompletionEvent::StreamChunk(chunk) => assert_eq!(chunk.index, 0),
            _ => panic!("Expected a stream chunk"),
        }
    }

    #[test]
    fn completion_summary_event_is_parsed() {
        // Given some bytes with a stream summary and a completion summary
        let bytes = b"data: {\"type\":\"stream_summary\",\"index\":0,\"model_version\":\"2022-04\",\"finish_reason\":\"maximum_tokens\"}\n\ndata: {\"type\":\"completion_summary\",\"num_tokens_prompt_total\":1,\"num_tokens_generated\":7}\n\n";

        // When they are parsed
        let events = parse_stream_event::<CompletionEvent>(bytes);

        // Then the first event is a stream summary and the last event is a completion summary
        let first = events.first().unwrap().as_ref().unwrap();
        match first {
            CompletionEvent::StreamSummary(summary) => {
                assert_eq!(summary.finish_reason, "maximum_tokens")
            }
            _ => panic!("Expected a completion summary"),
        }
        let second = events.last().unwrap().as_ref().unwrap();
        match second {
            CompletionEvent::CompletionSummary(summary) => {
                assert_eq!(summary.num_tokens_generated, 7)
            }
            _ => panic!("Expected a completion summary"),
        }
    }

    #[test]
    fn chat_stream_chunk_event_is_parsed() {
        // Given some bytes
        let bytes = b"data: {\"id\":\"831e41b4-2382-4b08-990e-0a3859967f43\",\"choices\":[{\"finish_reason\":null,\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"\"},\"logprobs\":null}],\"created\":1729782822,\"model\":\"pharia-1-llm-7b-control\",\"system_fingerprint\":null,\"object\":\"chat.completion.chunk\",\"usage\":null}\n\n";

        // When they are parsed
        let events = parse_stream_event::<ChatEvent>(bytes);
        let event = events.first().unwrap().as_ref().unwrap();

        // Then the event is a chat stream chunk
        assert_eq!(event.choices[0].delta.role.as_ref().unwrap(), "assistant");
    }

    #[test]
    fn chat_stream_chunk_without_role_is_parsed() {
        // Given some bytes without a role
        let bytes = b"data: {\"id\":\"a3ceca7f-32b2-4a6c-89e7-bc8eb5327f76\",\"choices\":[{\"finish_reason\":null,\"index\":0,\"delta\":{\"content\":\"Hello! How can I help you today? If you have any questions or need assistance, feel free to ask.\"},\"logprobs\":null}],\"created\":1729784197,\"model\":\"pharia-1-llm-7b-control\",\"system_fingerprint\":null,\"object\":\"chat.completion.chunk\",\"usage\":null}\n\n";

        // When they are parsed
        let events = parse_stream_event::<ChatEvent>(bytes);
        let event = events.first().unwrap().as_ref().unwrap();

        // Then the event is a chat stream chunk
        assert_eq!(event.choices[0].delta.content, "Hello! How can I help you today? If you have any questions or need assistance, feel free to ask.");
    }
}
